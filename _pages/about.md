---
permalink: /
title: "About Me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am a PhD student working on large language models, knowledge distillation, and learning under constraints, with a focus on making foundation models more efficient, robust, and controllable.

My research spans LLM distillation, text-to-SQL, retrieval-augmented generation, reinforcement learning for reasoning, and distributed training systems.

I have conducted applied research in industry settings, including as an Applied Scientist Intern at Amazon AWS AI, where I worked on large-scale language model distillation and distributed training infrastructure, and as a Machine Learning Research Intern at CodaMetrix and Nokia Bell Labs. Across these roles, I have built scalable multi-GPU training systems and worked closely with production-level models, bridging theoretical research and real-world deployment.

I am broadly interested in problems at the intersection of foundation models, reasoning, and systems, particularly where principled learning objectives meet practical constraints. My long-term goal is to develop methods that make advanced AI models more reliable, efficient, and aligned with real-world requirements.